#!/usr/bin/env python3
"""
Clean Eating Agent ‚Äî Multi-Source Scraper v2
Sources: Kupi.cz + iLetaky.cz + AkcniCeny.cz with cross-verification.
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from datetime import datetime, date, timedelta
from typing import Optional, Dict, List, Tuple
import time
import os

STORES = {"lidl": "Lidl", "kaufland": "Kaufland", "penny-market": "Penny Market", "billa": "Billa", "albert": "Albert"}
FLYER_CYCLES = {"Lidl": {"start_day": 0, "duration": 7}, "Kaufland": {"start_day": 3, "duration": 7}, "Penny Market": {"start_day": 2, "duration": 7}, "Billa": {"start_day": 2, "duration": 7}, "Albert": {"start_day": 2, "duration": 7}}

BANNED_KEYWORDS = ["p√°rky","p√°rk≈Ø","p√°rek","klob√°s","sal√°m","≈°unk","pa≈°tik","pa≈°tƒõt","≈°pek√°ƒç","vu≈ôt","bu≈ôt","slanin","mortadel","kabanos","jaternic","tlaƒçenk","jel√≠t","utopen","vysoƒçin","gothaj","debrec√≠n","piken","hot dog","bacon","chorizo","prosciutt","pancetta","ƒçokol√°d","su≈°enk","oplatk","chips","bramb≈Ørk","tyƒçink","bonbon","≈æel√©","gumov","drops","karamel","nug√°t","m√ºsli tyƒçink","proteinov","fitness tyƒçink","hotov√© j√≠dlo","pizza","lasagn","burger","nugget","kroket","hranolk","sma≈æen","obalovan","p≈ôedsma≈æen","keƒçup","tatarsk","majon√©z","dresing","limon√°d","cola","fant","sprite","energetick","energy","zmrzlin","nanuk","sorbet","toast","bageta","croissant","instantn√≠","pol√©vka s√°ƒçk","buj√≥n"]
PRIORITY_KEYWORDS = ["ku≈ôec√≠ prsn√≠","ku≈ôec√≠ prsa","ku≈ôe cel√©","kr≈Øt√≠","losos","tu≈à√°k","treska","tvaroh","jogurt ≈ôeck√Ω","skyr","cottage","vejce","brokolice","≈°pen√°t","rajƒçata","bor≈Øvk","ovesn√© vloƒçky","ƒçoƒçka","mandle","olivov√Ω olej","avok√°do","bat√°ty"]

HEADERS = {"User-Agent": "Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15", "Accept": "text/html,application/xhtml+xml", "Accept-Language": "cs-CZ,cs;q=0.9"}

def is_clean(name):
    nl = name.lower()
    return not any(b in nl for b in BANNED_KEYWORDS)

def is_priority(name):
    nl = name.lower()
    return any(k.lower() in nl for k in PRIORITY_KEYWORDS)

def get_clean_category(name):
    nl = name.lower()
    cats = {"meat": ["ku≈ôec√≠","kr≈Øt√≠","hovƒõz√≠","vep≈ôov","telec√≠","jehnƒõƒç√≠","kachn√≠","ku≈ôe"], "fish": ["losos","tu≈à√°k","tresk","pstruh","makrela","fil√©","ryb"], "dairy": ["tvaroh","jogurt","skyr","mozzarell","cottage","vejce","vajec","m√°slo","s√Ωr","eidam","gouda","ml√©ko","smetana","kef√≠r"], "produce": ["jablk","ban√°n","pomeranƒç","rajƒçat","paprik","okurk","mrkev","brokolice","≈°pen√°t","kvƒõt√°k","cuketa","bor≈Øvk","mal√≠n","hrozn","citron","kiwi","mango","avok√°d","celer","zelen√≠","cibule","ƒçesnek","bat√°t"], "pantry": ["olivov√Ω","r√Ω≈æe","ƒçoƒçk","fazol","hr√°ch","cizrn","ovesn√©","pohanka","o≈ôech","mandle","vla≈°sk","konzerv","tƒõstovin","med"]}
    for cat, kws in cats.items():
        if any(kw in nl for kw in kws): return cat
    return "other"

def get_validity_dates(store_name):
    today = date.today()
    cycle = FLYER_CYCLES.get(store_name, {"start_day": 2, "duration": 7})
    days_since = (today.weekday() - cycle["start_day"]) % 7
    start = today - timedelta(days=days_since)
    end = start + timedelta(days=cycle["duration"] - 1)
    return start.isoformat(), end.isoformat()

# ============================================================
# BIO AUDIT + NUTRITION
# ============================================================
BIO = {
    "meat": {"ku≈ôec√≠ prs": (92,8,9,9,"ƒåist√© dr≈Øbe≈æ√≠ maso, bohat√© na tryptofan","N√≠zk√Ω nasycen√© tuky, vysok√© b√≠lkoviny","Vysok√Ω TEF ~25%"), "ku≈ôe cel": (85,7,7,8,"Kolagen a glycin","V√≠ce ≈æeleza a zinku, k≈Ø≈æe zvy≈°uje tuky","Kompletn√≠ aminokyseliny, B-vitam√≠ny"), "kr≈Øt√≠": (93,8,9,9,"Nejlibovƒõj≈°√≠ dr≈Øbe≈æ√≠ maso","T√©mƒõ≈ô nulov√Ω tuk","Selen a B-vitam√≠ny"), "hovƒõz√≠": (80,7,6,8,"Bohat√© na ≈æelezo a B12","Vy≈°≈°√≠ nasycen√© tuky ‚Äî max 2√ó t√Ωdnƒõ","B12, ≈æelezo, zinek"), "vep≈ôov": (72,6,5,7,"Thiamin (B1)","Vy≈°≈°√≠ nasycen√© tuky","Thiamin a selen"), "mlet": (70,6,5,7,"Z√°le≈æ√≠ na kvalitƒõ","St≈ôedn√≠ nasycen√© tuky","B√≠lkoviny, ≈æelezo, B12")},
    "fish": {"losos": (96,9,10,9,"Omega-3 protiz√°nƒõtliv√Ω √∫ƒçinek na mikrobiom","Omega-3 sni≈æuj√≠ triglyceridy a tlak","Vitam√≠n D, selen, astaxanthin"), "tu≈à√°k": (85,7,8,8,"Minim√°ln√≠ zpracov√°n√≠, pozor na sod√≠k","Omega-3, niacin sni≈æuje LDL","Niacin a selen"), "tresk": (90,8,8,8,"Lehce straviteln√© fil√©","N√≠zkotuƒçn√° ryba","J√≥d, fosfor, selen"), "pstruh": (93,8,9,8,"Sladkovodn√≠ ryba s omega-3","N√≠zk√Ω tuk, vitam√≠n D","Kompletn√≠ b√≠lkoviny, selen")},
    "dairy": {"tvaroh": (94,9,7,9,"Probiotick√© kultury, kasein","Bohat√Ω na v√°pn√≠k a fosfor","Kasein = pomal√© aminokyseliny"), "jogurt": (91,9,7,8,"≈Ωiv√© kultury L. bulgaricus","Fermentace zlep≈°uje profil tuk≈Ø","Probiotika zlep≈°uj√≠ absorpci"), "vejce": (98,8,8,10,"Lecitin podporuje st≈ôevn√≠ bari√©ru","Diet√°rn√≠ cholesterol m√° minim√°ln√≠ vliv","Cholin, vitam√≠n D, B12, selen"), "mozzarell": (88,7,7,7,"Fermentovan√Ω s√Ωr","St≈ôedn√≠ tuky, bohat√Ω na v√°pn√≠k","V√°pn√≠k podporuje lipol√Ωzu"), "m√°slo": (78,6,5,7,"Butyr√°t ‚Äî ≈æivina pro st≈ôevo","Vysok√Ω nasycen√© tuky","Vitam√≠ny A, D, E, K2"), "cottage": (93,8,8,9,"Probiotick√© kultury, kasein","N√≠zk√Ω tuk","Pomal√Ω kasein, ide√°ln√≠ p≈ôed span√≠m"), "skyr": (94,9,8,9,"Islandsk√Ω fermentovan√Ω produkt","N√≠zk√Ω tuk, vysok√© b√≠lkoviny","2-3√ó v√≠ce b√≠lkovin ne≈æ jogurt"), "kef√≠r": (92,10,7,8,"60+ druh≈Ø probiotik","Probiotika pro c√©vy","Zlep≈°uje absorpci v√°pn√≠ku")},
    "produce": {"jablk": (100,9,8,7,"Pektin a polyfenoly ‚Äî top prebiotikum","Kvercet√≠n chr√°n√≠ cievy","N√≠zk√Ω GI, vl√°knina"), "ban√°n": (100,9,8,7,"Rezistentn√≠ ≈°krob je prebiotikum","Drasl√≠k reguluje tlak","Rychl√Ω zdroj energie"), "pomeranƒç": (100,9,9,8,"Pektin ‚Äî rozpustn√° vl√°knina","Vitam√≠n C, hesperidin","Vitam√≠n C zvy≈°uje absorpci ≈æeleza"), "rajƒçat": (100,8,9,8,"Lykopen podporuje mikrobiom","Lykopen chr√°n√≠ c√©vy","Ultra-n√≠zkokalorick√©"), "brokolice": (100,9,9,9,"Sulforafan a indol-3-karbinol","Protiz√°nƒõtliv√© √∫ƒçinky","Sulforafan aktivuje Nrf2"), "≈°pen√°t": (100,9,10,9,"Polyfenoly ≈æiv√≠ prospƒõ≈°n√© bakt√©rie","Nitr√°ty ‚Üí oxid dusnat√Ω","≈Ωelezo, fol√°t, vitam√≠n K"), "avok√°d": (98,8,9,8,"Vl√°knina a polyfenoly","Mononenasycen√© tuky sni≈æuj√≠ LDL","Drasl√≠k, vl√°knina, zdrav√© tuky"), "mrkev": (100,8,8,8,"Vl√°knina a beta-karoten","Beta-karoten a luteolin","Beta-karoten ‚Üí vitam√≠n A"), "paprik": (100,8,9,8,"Vitam√≠n C a vl√°knina","Vitam√≠n C 127mg/100g","Kapsaic√≠n zvy≈°uje termogen√©zu"), "cibule": (100,9,8,7,"Inul√≠n a FOS ‚Äî siln√© prebiotikum","Kvercet√≠n a s√≠ra chr√°n√≠ cievy","Chr√≥m podporuje inzul√≠n"), "ƒçesnek": (100,9,9,8,"Allic√≠n ‚Äî prebiotick√© a antimikrobi√°lne","Allic√≠n zni≈æuje tlak a LDL","Selenoaminokyseliny"), "bor≈Øvk": (100,9,9,8,"Antokyany ‚Äî mikrobiotick√° diverzita","Siln√© antioxidanty, ochrana DNA","N√≠zkokalorick√©, flavonoidy"), "kiwi": (100,8,8,8,"Actinidin usnad≈àuje tr√°ven√≠","Vitam√≠n C chr√°n√≠ c√©vy","Nejvy≈°≈°√≠ zdroj vitam√≠nu C"), "okurk": (100,7,7,7,"Vysok√Ω obsah vody","Drasl√≠k a ho≈ôƒç√≠k","Ultra-n√≠zkokalorick√°"), "hrozn": (95,8,9,7,"Resveratrol podporuje diverzitu","Resveratrol chr√°n√≠ c√©vy","P≈ô√≠rodn√≠ cukry s polyfenoly"), "cuketa": (100,7,7,7,"Vl√°knina a voda","Drasl√≠k reguluje tlak","Ultra-n√≠zkokalorick√°"), "citron": (100,8,9,8,"Pektin a polyfenoly","Vitam√≠n C chr√°n√≠ c√©vy","Podporuje absorpci ≈æeleza"), "malin": (100,9,9,8,"Ellagitaniny ‚Äî siln√© prebiotikum","Antioxidanty chr√°n√≠ c√©vy","N√≠zkokalorick√©, vysok√° vl√°knina")},
    "pantry": {"olivov√Ω": (95,8,10,8,"Polyfenoly podporuj√≠ prospƒõ≈°n√© bakt√©rie","Kyselina olejov√° ‚Äî z√°klad st≈ôedomo≈ôsk√© diety","Mononenasycen√© tuky zlep≈°uj√≠ inzul√≠n"), "ovesn√©": (95,10,10,9,"Beta-glukan ‚Äî zlat√Ω standard prebiot√≠k","3g beta-glukanu sni≈æuje LDL o 5-10%","N√≠zk√Ω GI, pomal√© uvol≈àov√°n√≠"), "ƒçoƒçk": (95,10,9,9,"Rezistentn√≠ ≈°krob + oligosacharidy","Rozpustn√° vl√°knina sni≈æuje LDL","N√≠zk√Ω GI, vysok√© b√≠lkoviny a ≈æelezo"), "mandle": (98,9,10,9,"Vl√°knina a polyfenoly","30g o≈ôech≈Ø sni≈æuje KV riziko o 30%","Zdrav√© tuky, ho≈ôƒç√≠k, vitam√≠n E"), "r√Ω≈æe": (80,6,6,7,"N√≠zk√Ω obsah vl√°kniny","Neutr√°ln√≠ vliv","Basmati m√° ni≈æ≈°√≠ GI"), "tƒõstovin": (82,7,7,7,"Celozrnn√© ‚Äî vl√°knina","Celozrnn√© maj√≠ ni≈æ≈°√≠ GI","Pomal√© uvol≈àov√°n√≠ energie"), "fazol": (88,9,8,8,"Lu≈°tƒõniny ‚Äî TOP prebiotikum","Rozpustn√° vl√°knina sni≈æuje LDL","N√≠zk√Ω GI, b√≠lkoviny"), "med": (75,7,5,5,"Oligosacharidy s prebiotick√Ωm √∫ƒçinkom","Vysok√Ω cukr ‚Äî max 1 ly≈æica","Lep≈°ia alternat√≠va k cukru"), "o≈ôech": (97,9,10,9,"Polyfenoly a vl√°knina","Omega-3 (ALA)","Zdrav√© tuky, ho≈ôƒç√≠k")}
}

NUTRI = {"ku≈ôec√≠ prs": (110,23.1,0,1.2,0), "ku≈ôe cel": (167,20,0,9.3,0), "ku≈ôec√≠ steh": (177,18.2,0,11.2,0), "kr≈Øt√≠": (104,24.6,0,0.7,0), "hovƒõz√≠": (250,26,0,15,0), "vep≈ôov": (186,18.5,0,12.2,0), "mlet": (145,20,0,7,0), "losos": (208,20.4,0,13.4,0), "tu≈à√°k": (116,25.5,0,1,0), "tresk": (82,17.6,0,0.7,0), "pstruh": (119,20.5,0,3.5,0), "tvaroh": (130,12.8,3.1,7.5,0), "jogurt": (72,3.5,4.8,3.8,0), "≈ôeck√Ω": (97,9,3.5,5,0), "vejce": (143,12.6,0.7,9.9,0), "mozzarell": (254,18.5,1,19.5,0), "m√°slo": (717,0.6,0.8,81,0), "cottage": (98,11,3.3,4,0), "skyr": (63,11,4,0.2,0), "kef√≠r": (56,3.3,4.7,1.5,0), "jablk": (52,0.3,13.8,0.2,2.4), "ban√°n": (89,1.1,22.8,0.3,2.6), "pomeranƒç": (47,0.9,11.8,0.1,2.4), "rajƒçat": (18,0.9,3.9,0.2,1.2), "paprik": (31,1,6,0.3,2.1), "brokolice": (34,2.8,7,0.4,2.6), "≈°pen√°t": (23,2.9,3.6,0.4,2.2), "mrkev": (41,0.9,9.6,0.2,2.8), "okurk": (15,0.7,3.6,0.1,0.5), "avok√°d": (160,2,9,15,6.7), "cibule": (40,1.1,9.3,0.1,1.7), "ƒçesnek": (149,6.4,33.1,0.5,2.1), "bor≈Øvk": (57,0.7,14.5,0.3,2.4), "kiwi": (63,1.1,15.4,0.3,2), "hrozn": (69,0.7,18.1,0.2,0.9), "cuketa": (17,1.2,3.1,0.3,1), "olivov√Ω": (884,0,0,100,0), "ovesn√©": (372,13.5,58.7,7,10.6), "r√Ω≈æe": (350,7,78,0.6,1), "ƒçoƒçk": (353,25.4,60.1,1.1,10.7), "mandle": (576,21.2,21.7,49.4,12.2), "tƒõstovin": (348,13.5,65,2.5,7.5), "fazol": (81,4.6,12.9,0.5,3.7), "med": (304,0.3,76,0,0), "o≈ôech": (654,15,14,65,6.7)}

def get_bio_audit(name, cat):
    nl = name.lower()
    tpls = BIO.get(cat, {})
    for kw, d in tpls.items():
        if kw in nl:
            return d[0], {"microbiome": {"score": d[1], "detail": d[4]}, "cardiovascular": {"score": d[2], "detail": d[5]}, "metabolism": {"score": d[3], "detail": d[6]}}
    defaults = {"meat": 75, "fish": 90, "dairy": 85, "produce": 100, "pantry": 80, "other": 60}
    s = defaults.get(cat, 70)
    return s, {"microbiome": {"score": 7, "detail": "Standardn√≠ produkt"}, "cardiovascular": {"score": 7, "detail": "Neutr√°ln√≠ vliv"}, "metabolism": {"score": 7, "detail": "Standardn√≠ nutriƒçn√≠ profil"}}

def get_nutrition(name):
    nl = name.lower()
    for kw, n in NUTRI.items():
        if kw in nl: return {"kcal": n[0], "protein": n[1], "carbs": n[2], "fat": n[3], "fiber": n[4]}
    return {"kcal": 0, "protein": 0, "carbs": 0, "fat": 0, "fiber": 0}

# ============================================================
# SCRAPERS
# ============================================================

def scrape_kupi_sleva(slug):
    url = f"https://www.kupi.cz/sleva/{slug}"
    result = {"source": "kupi.cz", "slug": slug, "url": url, "name": "", "regular_price": None, "offers": [], "best_price": None, "max_discount": None}
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code != 200: return result
        soup = BeautifulSoup(resp.text, "html.parser")
        text = soup.get_text()
        h1 = soup.select_one("h1")
        if h1: result["name"] = h1.get_text(strip=True)
        bezna = re.search(r'bƒõ≈ænƒõ\s+stoj√≠\s+(\d+[,.]?\d*)\s*Kƒç', text)
        if bezna: result["regular_price"] = float(bezna.group(1).replace(",", "."))
        nejl = re.search(r'Nejv√Ωhodnƒõji.*?(\d+[,.]?\d*)\s*Kƒç', text)
        if nejl: result["best_price"] = float(nejl.group(1).replace(",", "."))
        sleva = re.findall(r'[‚Äì-](\d+)\s*%', text)
        if sleva: result["max_discount"] = max(int(s) for s in sleva)
        for ss, sn in STORES.items():
            pat = re.compile(rf'({re.escape(sn)}).*?cena\s*(\d+[,.]?\d*)\s*Kƒç', re.IGNORECASE | re.DOTALL)
            for m in pat.finditer(text):
                p = float(m.group(2).replace(",", "."))
                if not any(o["store"]==sn and o["sale_price"]==p for o in result["offers"]):
                    vf, vu = get_validity_dates(sn)
                    result["offers"].append({"store": sn, "sale_price": p, "valid_from": vf, "valid_until": vu, "source": "kupi.cz"})
            pat2 = re.compile(rf'({re.escape(sn)}).*?(\d+[,.]?\d*)\s*Kƒç\s*/\s*(\d+\s*(?:kg|g|ks|l|ml))', re.IGNORECASE | re.DOTALL)
            for m in pat2.finditer(text):
                p = float(m.group(2).replace(",", "."))
                u = m.group(3).strip()
                if not any(o["store"]==sn and o["sale_price"]==p for o in result["offers"]):
                    vf, vu = get_validity_dates(sn)
                    result["offers"].append({"store": sn, "sale_price": p, "unit": u, "valid_from": vf, "valid_until": vu, "source": "kupi.cz"})
        for o in result["offers"]:
            plat = re.search(rf'{re.escape(o["store"])}.*?plat√≠\s+do\s+\w+\s+(\d+)\.\s*(\d+)\.', text, re.IGNORECASE | re.DOTALL)
            if plat:
                d, mo = int(plat.group(1)), int(plat.group(2))
                o["valid_until"] = f"{date.today().year}-{mo:02d}-{d:02d}"
        return result
    except Exception as e:
        print(f"  ‚ö† kupi.cz [{slug}]: {e}")
        return result

def scrape_iletaky(query):
    url = f"https://www.iletaky.cz/hledani/?q={requests.utils.quote(query)}"
    results = []
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        if resp.status_code != 200: return results
        text = BeautifulSoup(resp.text, "html.parser").get_text()
        for ps in re.findall(r'(\d+[,.]?\d*)\s*Kƒç', text)[:5]:
            p = float(ps.replace(",", "."))
            if 1 < p < 1000: results.append({"source": "iletaky.cz", "price": p})
        return results
    except: return results

def scrape_akcniceny(query):
    url = f"https://www.akcniceny.cz/hledani/?q={requests.utils.quote(query)}"
    results = []
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        if resp.status_code != 200: return results
        text = BeautifulSoup(resp.text, "html.parser").get_text()
        for ps in re.findall(r'(\d+[,.]?\d*)\s*Kƒç', text)[:5]:
            p = float(ps.replace(",", "."))
            if 1 < p < 1000: results.append({"source": "akcniceny.cz", "price": p})
        return results
    except: return results

def cross_verify(name, kupi_price):
    v = {"kupi_price": kupi_price, "other_sources": [], "verified": False, "confidence": "low"}
    il = scrape_iletaky(name)
    for i in il:
        v["other_sources"].append(i)
        if abs(i["price"] - kupi_price) < 2.0: v["verified"] = True
    time.sleep(0.3)
    ac = scrape_akcniceny(name)
    for i in ac:
        v["other_sources"].append(i)
        if abs(i["price"] - kupi_price) < 2.0: v["verified"] = True
    v["confidence"] = "high" if v["verified"] else ("medium" if v["other_sources"] else "low")
    return v

# ============================================================
# PRODUCT SLUGS
# ============================================================
SLUGS = {
    "meat": ["kureci-prsni-rizky","kureci-prsa","kureci-stehna","kure","kruti-prsni-rizky","kruti-prsa","hovezi-zadni","veprova-plec","mleta-smes"],
    "fish": ["losos-filety","losos-obecny-filety","tunak-steak-franz-josef","file-z-aljasske-tresky","pstruh-duhovy"],
    "dairy": ["vejce-m","vejce-s","maslo-madeta","maslo-ceske","tvaroh-jihocesky-madeta","tvaroh-polotucny-jihocesky-madeta","tvaroh-tucny-karlova-koruna","jogurt-recky","selsky-jogurt-hollandia","mozzarella-pilos","mozzarella-galbani","cottage-cheese","skyr-milko"],
    "produce": ["jablka-cervena","banany","pomerance","citrony","boruvky","maliny","rajcata","papriky","brokolice","spenat","mrkev","cuketa","avokado","okurka-salatova","cibule","cesnek","kiwi","hrozny"],
    "pantry": ["olivovy-olej-bertolli","olivovy-olej-extra-virgin","ryze-basmati","ovesne-vlocky","ovesne-vlocky-emco","cocka-cervena","mandle","vlaske-orechy","testoviny-barilla","fazole-bile","med"],
}

# ============================================================
# MAIN
# ============================================================

def run_full_scrape():
    print("=" * 60)
    print("üî¨ CLEAN EATING AGENT ‚Äî Multi-Source Scraper v2")
    print(f"üìÖ {datetime.now().strftime('%d.%m.%Y %H:%M')}")
    print("üì° Kupi.cz + iLetaky.cz + AkcniCeny.cz")
    print("=" * 60)
    
    products = []
    stats = {"total": 0, "clean": 0, "priority": 0, "with_price": 0, "verified": 0}
    
    for cat, slugs in SLUGS.items():
        print(f"\nüì¶ {cat.upper()}")
        print("-" * 40)
        for slug in slugs:
            time.sleep(0.5)
            stats["total"] += 1
            data = scrape_kupi_sleva(slug)
            if not data["name"]:
                print(f"  ‚è≠ {slug} ‚Äî nen√°jden√©")
                continue
            if not is_clean(data["name"]):
                print(f"  üö´ {data['name']} ‚Äî UPF")
                continue
            stats["clean"] += 1
            c = get_clean_category(data["name"])
            pri = is_priority(data["name"])
            if pri: stats["priority"] += 1
            if data.get("offers") or data.get("best_price"): stats["with_price"] += 1
            
            ver = None
            if data.get("best_price"):
                time.sleep(0.3)
                ver = cross_verify(data["name"], data["best_price"])
                if ver["verified"]: stats["verified"] += 1
            
            score, bio = get_bio_audit(data["name"], c)
            nutri = get_nutrition(data["name"])
            
            p = {
                "name": data["name"], "slug": slug, "category": c, "is_priority": pri,
                "regular_price": data.get("regular_price"), "best_price": data.get("best_price"),
                "max_discount": data.get("max_discount"), "offers": data.get("offers", []),
                "clean_score": score, "bio_audit": bio, "nutrition": nutri,
                "source_url": data["url"],
                "sources_checked": list(set(["kupi.cz"] + [s["source"] for s in (ver or {}).get("other_sources", [])])),
                "verification": ver, "scraped_at": datetime.now().isoformat(),
            }
            products.append(p)
            
            ps = f" ‚Üí {data['best_price']} Kƒç" if data.get("best_price") else (f" ‚Üí od {min(o['sale_price'] for o in data['offers'])} Kƒç" if data.get("offers") else "")
            ds = f" (-{data['max_discount']}%)" if data.get("max_discount") else ""
            vs = " ‚úì" if ver and ver.get("verified") else ""
            print(f"  ‚úÖ {data['name']}{ps}{ds}{' ‚≠ê' if pri else ''}{vs}")
    
    print(f"\n{'='*60}\nüìä V√ùSLEDKY\n  Spracovan√Ωch: {stats['total']}\n  Clean: {stats['clean']}\n  Prioritn√Ωch: {stats['priority']}\n  S cenou: {stats['with_price']}\n  Cross-overen√Ωch: {stats['verified']}\n{'='*60}")
    return products

def save_results(products, output_dir="."):
    os.makedirs(output_dir, exist_ok=True)
    fp = os.path.join(output_dir, "products.json")
    out = {"generated_at": datetime.now().isoformat(), "flyer_week": f"{date.today().strftime('%d.%m')} ‚Äì {(date.today()+timedelta(days=6)).strftime('%d.%m.%Y')}", "store_location": "P≈ô√≠bram", "sources": ["kupi.cz","iletaky.cz","akcniceny.cz"], "total_products": len(products), "products": products}
    with open(fp, "w", encoding="utf-8") as f: json.dump(out, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ {fp}")
    
    cats = {}
    for p in products:
        cats.setdefault(p["category"], []).append(p)
    for c, items in cats.items():
        cp = os.path.join(output_dir, f"products_{c}.json")
        with open(cp, "w", encoding="utf-8") as f: json.dump({"generated_at": datetime.now().isoformat(), "category": c, "total_products": len(items), "products": items}, f, ensure_ascii=False, indent=2)
        print(f"üíæ {cp} ({len(items)})")
    
    sp = os.path.join(output_dir, "summary.json")
    summary = {"generated_at": datetime.now().isoformat(), "store_location": "P≈ô√≠bram", "sources": ["kupi.cz","iletaky.cz","akcniceny.cz"], "categories": {c: {"count": len(i), "min_price": min((p["best_price"] for p in i if p.get("best_price")), default=None), "avg_discount": round(sum(p.get("max_discount") or 0 for p in i)/max(len(i),1), 1)} for c, i in cats.items()}, "total_clean_products": len(products), "total_with_offers": sum(1 for p in products if p.get("offers") or p.get("best_price")), "total_verified": sum(1 for p in products if p.get("verification", {}).get("verified"))}
    with open(sp, "w", encoding="utf-8") as f: json.dump(summary, f, ensure_ascii=False, indent=2)
    print(f"üíæ {sp}")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--output", default="data")
    args = parser.parse_args()
    products = run_full_scrape()
    save_results(products, output_dir=args.output)
    print("\n‚úÖ Hotovo!")
